{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6937a96",
   "metadata": {},
   "source": [
    "# ðŸ§  Deepfake Detection using EfficientNetB4\n",
    "This notebook extracts frames from Celeb-DF v2 videos and trains an EfficientNetB4 model to classify them as real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the path to your zip file in Google Drive\n",
    "zip_path = \"/content/drive/MyDrive/deepfake_project1/archive.zip\"\n",
    "extract_path = \"/content/celeb-df-v2\"\n",
    "\n",
    "# Unzip the dataset\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(\"âœ… Dataset extracted to:\", extract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Create output directories\n",
    "real_frame_dir = \"/content/frames/real\"\n",
    "fake_frame_dir = \"/content/frames/fake\"\n",
    "os.makedirs(real_frame_dir, exist_ok=True)\n",
    "os.makedirs(fake_frame_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract 1 frame per video\n",
    "def extract_frames(video_dir, output_dir, label):\n",
    "    count = 0\n",
    "    for filename in os.listdir(video_dir):\n",
    "        if not filename.lower().endswith('.mp4'):\n",
    "            continue\n",
    "        video_path = os.path.join(video_dir, filename)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        success, frame = cap.read()\n",
    "        if success:\n",
    "            frame_path = os.path.join(output_dir, f\"{label}_{count}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            count += 1\n",
    "        cap.release()\n",
    "\n",
    "# Extract from Celeb-real and Celeb-synthesis\n",
    "extract_frames(\"/content/celeb-df-v2/Celeb-real\", real_frame_dir, \"real\")\n",
    "extract_frames(\"/content/celeb-df-v2/Celeb-synthesis\", fake_frame_dir, \"fake\")\n",
    "print(\"âœ… Frame extraction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b603fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Destination folders\n",
    "train_real = \"/content/frames/train/real\"\n",
    "train_fake = \"/content/frames/train/fake\"\n",
    "val_real = \"/content/frames/val/real\"\n",
    "val_fake = \"/content/frames/val/fake\"\n",
    "\n",
    "# Create required folders\n",
    "for folder in [train_real, train_fake, val_real, val_fake]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Helper function to split and copy\n",
    "def split_and_copy(source_dir, train_dir, val_dir, split_ratio=0.8):\n",
    "    files = os.listdir(source_dir)\n",
    "    random.shuffle(files)\n",
    "    split_index = int(len(files) * split_ratio)\n",
    "    train_files = files[:split_index]\n",
    "    val_files = files[split_index:]\n",
    "\n",
    "    for f in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, f), os.path.join(train_dir, f))\n",
    "    for f in val_files:\n",
    "        shutil.copy(os.path.join(source_dir, f), os.path.join(val_dir, f))\n",
    "\n",
    "# Apply split to both real and fake\n",
    "split_and_copy(\"/content/frames/real\", train_real, val_real)\n",
    "split_and_copy(\"/content/frames/fake\", train_fake, val_fake)\n",
    "print(\"âœ… Frame split complete â€” Train/Val folders are now ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "img_size = 380  # EfficientNetB4 input size\n",
    "batch_size = 16\n",
    "\n",
    "train_dir = '/content/frames/train'\n",
    "val_dir = '/content/frames/val'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=10,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load EfficientNetB4 with pre-trained ImageNet weights, exclude the top layer\n",
    "base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Add custom classification layers on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)  # Binary classification\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = \"/content/drive/MyDrive/deepfake_project1/best_model_b4.h5\"\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
